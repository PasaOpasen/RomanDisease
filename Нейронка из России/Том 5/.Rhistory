# This holds our extracted sequences
sentences <- str_sub(text, text_indexes, text_indexes + maxlen - 1)
# This holds the targets (the follow-up characters)
next_chars <- str_sub(text, text_indexes + maxlen, text_indexes + maxlen)
cat("Number of sequences: ", length(sentences), "\n")
# List of unique characters in the corpus
chars <- unique(sort(strsplit(text, "")[[1]]))
cat("Unique characters:", length(chars), "\n")
chars
source('C:/Users/qtckp/OneDrive/Рабочий стол/RomanDisease/Нейронка из России/Том 5/tokenize.R')
source('C:/Users/qtckp/OneDrive/Рабочий стол/RomanDisease/Нейронка из России/Том 5/base_model.R')
model <- keras_model_sequential() %>%
layer_lstm(units = 128, input_shape = c(maxlen, length(chars))) %>%
layer_dense(units = length(chars), activation = "softmax")
optimizer <- optimizer_rmsprop(lr = 0.01)
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer
)
model
sample_next_char <- function(preds, temperature = 1.0) {
preds <- as.numeric(preds)
preds <- log(preds) / temperature
exp_preds <- exp(preds)
preds <- exp_preds / sum(exp_preds)
which.max(t(rmultinom(1, 1, preds)))
}
for (epoch in 1:60) {
cat("epoch", epoch, "\n")
# Fit the model for 1 epoch on the available training data
model %>% fit(x, y, batch_size = 128, epochs = 1)
# Select a text seed at random
start_index <- sample(1:(nchar(text) - maxlen - 1), 1)
seed_text <- str_sub(text, start_index, start_index + maxlen - 1)
cat("--- Generating with seed:", seed_text, "\n\n")
for (temperature in c(0.2, 0.5, 1.0, 1.2)) {
cat("------ temperature:", temperature, "\n")
cat(seed_text, "\n")
generated_text <- seed_text
# We generate 400 characters
for (i in 1:400) {
sampled <- array(0, dim = c(1, maxlen, length(chars)))
generated_chars <- strsplit(generated_text, "")[[1]]
for (t in 1:length(generated_chars)) {
char <- generated_chars[[t]]
sampled[1, t, char_indices[[char]]] <- 1
}
preds <- model %>% predict(sampled, verbose = 0)
next_index <- sample_next_char(preds[1,], temperature)
next_char <- chars[[next_index]]
generated_text <- paste0(generated_text, next_char)
generated_text <- substring(generated_text, 2)
cat(next_char)
}
cat("\n\n")
}
}
library(keras)
load('arrays.rdata')
source('C:/Users/qtckp/OneDrive/Рабочий стол/RomanDisease/Нейронка из России/Том 5/fitting2.R')
source('C:/Users/qtckp/OneDrive/Рабочий стол/RomanDisease/Нейронка из России/Том 5/fitting2.R')
source('C:/Users/qtckp/OneDrive/Рабочий стол/RomanDisease/Нейронка из России/Том 5/fitting.R')
source('C:/Users/qtckp/OneDrive/Рабочий стол/RomanDisease/Нейронка из России/Том 5/fitting2.R')
source('C:/Users/qtckp/OneDrive/Рабочий стол/RomanDisease/Нейронка из России/Том 5/fitting2.R')
model <- keras_model_sequential() %>%
layer_lstm(units = 256, input_shape = c(maxlen, length(chars)),
recurrent_activation = "sigmoid", dropout = 0.3) %>%
layer_dense(units = length(chars), activation = "softmax")
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_rmsprop(lr = 0.01)
)
model
model %>%
fit(X_train, y_train,
batch_size = 128,
epochs = 20,
validation_data = list(X_test, y_test))
all_inds = sample(c(1,2,3), size = dim(x)[1], replace = T, prob = c(0.7,0.2,0.1))
train_inds = all_inds == 1
val_inds = all_inds == 2
test_inds = all_inds == 3
X_train = x[train_inds,,]
y_train = y[train_inds,]
X_test = x[test_inds,,]
y_test = y[test_inds,]
model <- keras_model_sequential() %>%
layer_lstm(units = 128, input_shape = c(maxlen, length(chars)),
recurrent_activation = "sigmoid") %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = length(chars), activation = "softmax")
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_rmsprop(lr = 0.01)
)
model %>%
fit(X_train, y_train,
batch_size = 128,
epochs = 20,
validation_data = list(X_test, y_test))
all_inds
model <- keras_model_sequential() %>%
layer_lstm(units = 128, input_shape = c(maxlen, length(chars)),
recurrent_activation = "sigmoid") %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = length(chars), activation = "softmax")
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_rmsprop(lr = 0.01)
)
model %>%
fit(X_train, y_train,
batch_size = 128,
epochs = 10,
validation_data = list(X_test, y_test))
source('C:/Users/qtckp/OneDrive/Рабочий стол/RomanDisease/Нейронка из России/Том 5/fitting2.R')
model <- keras_model_sequential() %>%
layer_lstm(units = 128, input_shape = c(maxlen, length(chars)),
recurrent_activation = "sigmoid") %>%
layer_batch_normalization() %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = length(chars), activation = "softmax")
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_rmsprop(lr = 0.01)
)
model %>%
fit(X_train, y_train,
batch_size = 128,
epochs = 20,
validation_data = list(X_test, y_test))
source('C:/Users/qtckp/OneDrive/Рабочий стол/RomanDisease/Нейронка из России/Том 5/fitting2.R')
source('C:/Users/qtckp/OneDrive/Рабочий стол/RomanDisease/Нейронка из России/Том 5/fitting2.R')
first_inds
source('C:/Users/qtckp/OneDrive/Рабочий стол/RomanDisease/Нейронка из России/Том 5/fitting2.R')
model <- keras_model_sequential() %>%
layer_lstm(units = 128, input_shape = c(maxlen, length(chars)),
recurrent_activation = "sigmoid") %>%
#layer_batch_normalization() %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = length(chars), activation = "softmax")
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_rmsprop(lr = 0.01)
)
model %>%
fit(X_train, y_train,
batch_size = 128,
epochs = 30,
validation_data = list(X_test, y_test),
callbacks = list(
callback_reduce_lr_on_plateau(
monitor = "val_loss",
patience = 5,
factor = 0.5
)
)
)
save_model_hdf5(model, 'fit2_30epochs_1.76val_loss.h5', overwrite = TRUE, include_optimizer = TRUE)
model %>%
fit(X_train, y_train,
batch_size = 128,
epochs = 30,
validation_data = list(X_test, y_test),
callbacks = list(
callback_reduce_lr_on_plateau(
monitor = "val_loss",
patience = 5,
factor = 0.5
)
)
)
model <- keras_model_sequential() %>%
layer_lstm(units = 128, input_shape = c(maxlen, length(chars)),
recurrent_activation = "sigmoid") %>%
#layer_batch_normalization() %>%
layer_dropout(rate = 0.7) %>%
layer_dense(units = length(chars), activation = "softmax")
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_rmsprop(lr = 0.02)
)
model
model %>%
fit(X_train, y_train,
batch_size = 128,
epochs = 30,
validation_data = list(X_test, y_test),
callbacks = list(
callback_reduce_lr_on_plateau(
monitor = "val_loss",
patience = 3,
factor = 0.5
)
)
)
model <- keras_model_sequential() %>%
layer_lstm(units = 128, input_shape = c(maxlen, length(chars)),
recurrent_activation = "sigmoid") %>%
#layer_batch_normalization() %>%
layer_dropout(rate = 0.7) %>%
layer_dense(units = length(chars), activation = "softmax")
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_rmsprop(lr = 0.015)
)
model %>%
fit(X_train, y_train,
batch_size = 128,
epochs = 50,
validation_data = list(X_test, y_test),
callbacks = list(
callback_reduce_lr_on_plateau(
monitor = "val_loss",
patience = 2,
factor = 0.7
)
)
)
model <- keras_model_sequential() %>%
layer_lstm(units = 128, input_shape = c(maxlen, length(chars)),
recurrent_activation = "sigmoid") %>%
#layer_batch_normalization() %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = length(chars), activation = "softmax")
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_rmsprop(lr = 0.02)
)
model %>%
fit(X_train, y_train,
batch_size = 128,
epochs = 50,
validation_data = list(X_test, y_test),
callbacks = list(
callback_reduce_lr_on_plateau(
monitor = "val_loss",
patience = 1,
factor = 0.7
)
)
)
model <- keras_model_sequential() %>%
layer_lstm(units = 128, input_shape = c(maxlen, length(chars)),
recurrent_activation = "sigmoid") %>%
#layer_batch_normalization() %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = length(chars), activation = "softmax")
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_rmsprop(lr = 0.1)
)
model %>%
fit(X_train, y_train,
batch_size = 128,
epochs = 35,
validation_data = list(X_test, y_test),
callbacks = list(
callback_reduce_lr_on_plateau(
monitor = "val_loss",
patience = 1,
factor = 0.7
)
)
)
model <- keras_model_sequential() %>%
layer_lstm(units = 128, input_shape = c(maxlen, length(chars)),
recurrent_activation = "sigmoid") %>%
#layer_batch_normalization() %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = length(chars), activation = "softmax")
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_rmsprop(lr = 0.05)
)
model %>%
fit(X_train, y_train,
batch_size = 128,
epochs = 35,
validation_data = list(X_test, y_test),
callbacks = list(
callback_reduce_lr_on_plateau(
monitor = "val_loss",
patience = 1,
factor = 0.7
)
)
)
model <- keras_model_sequential() %>%
layer_lstm(units = 128, input_shape = c(maxlen, length(chars)),
recurrent_activation = "sigmoid") %>%
#layer_batch_normalization() %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = length(chars), activation = "softmax")
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_rmsprop(lr = 0.03)
)
model %>%
fit(X_train, y_train,
batch_size = 128,
epochs = 35,
validation_data = list(X_test, y_test),
callbacks = list(
callback_reduce_lr_on_plateau(
monitor = "val_loss",
patience = 1,
factor = 0.7
)
)
)
model <- keras_model_sequential() %>%
layer_lstm(units = 128, input_shape = c(maxlen, length(chars)),
recurrent_activation = "sigmoid", return_sequences = T) %>%
layer_lstm(units = 64, recurrent_activation = "sigmoid") %>%
#layer_batch_normalization() %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = length(chars), activation = "softmax")
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_rmsprop(lr = 0.01)
)
model %>%
fit(X_train, y_train,
batch_size = 128,
epochs = 35,
validation_data = list(X_test, y_test),
callbacks = list(
callback_reduce_lr_on_plateau(
monitor = "val_loss",
patience = 1,
factor = 0.7
)
)
)
model <- keras_model_sequential() %>%
layer_lstm(units = 128, input_shape = c(maxlen, length(chars)),
recurrent_activation = "sigmoid", return_sequences = T, dropout = 0.4) %>%
layer_lstm(units = 128, recurrent_activation = "sigmoid") %>%
#layer_batch_normalization() %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = length(chars), activation = "softmax")
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_rmsprop(lr = 0.01)
)
model %>%
fit(X_train, y_train,
batch_size = 128,
epochs = 35,
validation_data = list(X_test, y_test),
callbacks = list(
callback_reduce_lr_on_plateau(
monitor = "val_loss",
patience = 1,
factor = 0.7
)
)
)
model <- keras_model_sequential() %>%
layer_lstm(units = 128, input_shape = c(maxlen, length(chars)),
recurrent_activation = "sigmoid", return_sequences = T, dropout = 0.4) %>%
layer_lstm(units = 128, recurrent_activation = "sigmoid") %>%
#layer_batch_normalization() %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = length(chars), activation = "softmax")
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_rmsprop(lr = 0.1)
)
model %>%
fit(X_train, y_train,
batch_size = 128,
epochs = 35,
validation_data = list(X_test, y_test),
callbacks = list(
callback_reduce_lr_on_plateau(
monitor = "val_loss",
patience = 1,
factor = 0.7
)
)
)
source('C:/Users/qtckp/OneDrive/Рабочий стол/RomanDisease/Нейронка из России/Том 5/tokenize.R')
warnings()
source('C:/Users/qtckp/OneDrive/Рабочий стол/RomanDisease/Нейронка из России/Том 5/base_model.R')
library(keras)
load('arrays 50 5.rdata')
#all_inds = sample(c(1,2,3), size = dim(x)[1], replace = T, prob = c(0.7,0.2,0.1))
first_inds = ifelse(1:dim(x)[1] > 0.3*dim(x)[1], 1, 2)
all_inds = sample(c(2,3), size = sum(first_inds==2), replace = T, prob = c(0.6,0.4))
first_inds[first_inds == 2] = all_inds
all_inds = first_inds
train_inds = all_inds == 1
val_inds = all_inds == 2
test_inds = all_inds == 3
X_train = x[train_inds,,]
y_train = y[train_inds,]
X_test = x[val_inds,,]
y_test = y[val_inds,]
model <- keras_model_sequential() %>%
layer_lstm(units = 128, input_shape = c(maxlen, length(chars)),
recurrent_activation = "sigmoid") %>%
#layer_batch_normalization() %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = length(chars), activation = "softmax")
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_rmsprop(lr = 0.01)
)
model %>%
fit(X_train, y_train,
batch_size = 128,
epochs = 30,
validation_data = list(X_test, y_test),
callbacks = list(
callback_reduce_lr_on_plateau(
monitor = "val_loss",
patience = 3,
factor = 0.5
)
)
)
model %>%
fit(X_train, y_train,
batch_size = 128,
epochs = 30,
validation_data = list(X_test, y_test),
callbacks = list(
callback_reduce_lr_on_plateau(
monitor = "val_loss",
patience = 3,
factor = 0.5
)
)
)
library(keras)
library(stringr)
text_path = 'Texts'
file_name = 'text.txt'
path = file.path(getwd(),text_path, file_name)
text <- tolower(readChar(path, file.info(path)$size))
cat("Text length:", nchar(text), "\n")
count =  nchar(text)
cat("Text length:",count, "\n")
if(count >= 60){
sentence = text[(count-60):count]
}else{
how = 60-count
sentence = paste0(rep(' ',how), text)
}
if(count >= 60){
sentence = text[(count-60):count]
}else{
how = 60-count
sentence = paste0(paste0(rep(' ',how)), text)
}
paste0(rep(' ',how))
q = paste0(rep(' ',how))
q[1]
q[2]
paste0(rep(' ',how), sep='')
paste("A", 1:6, sep = "")
paste("the", "quick", "brown", "fox", "jumps", sep=" ")
paste(rep(' ',how), sep=" ")
paste0(rep("2",how))
if(count >= 60){
sentence = text[(count-60):count]
}else{
how = 60-count
sentence = paste0(paste0(rep(' ',how), collapse = ''), text)
}
if(count >= 60){
sentence = text[(count-60):count]
}else{
how = 60-count
sentence = paste0(paste0(rep('',how), collapse = ' '), text)
}
nchar(sentence)
if(count >= 60){
sentence = text[(count-60):count]
}else{
how = 60-count
sentence = paste0(paste0(rep('',how+1), collapse = ' '), text)
}
nchar(sentence)
source('C:/Users/qtckp/OneDrive/Рабочий стол/RomanDisease/Нейронка из России/Том 5/continue_text.R')
sentence
sampled[1,,]
rowSums(sampled[1,,])
text <- tolower(readChar(path, file.info(path)$size))
generated_chars <- strsplit(sentence, "")[[1]]
generated_chars
readLines(path)
readLines(path, encoding = 'utf8')
readChar(path, file.info(path)$size)
readLines(path, encoding = 'utf8')
readLines(path, encoding = 'utf8')
readLines(path, encoding = 'utf-8')
install.packages("tidyverse")
readr::read_lines(path)
source('C:/Users/qtckp/OneDrive/Рабочий стол/RomanDisease/Нейронка из России/Том 5/continue_text.R')
source('C:/Users/qtckp/OneDrive/Рабочий стол/RomanDisease/Нейронка из России/Том 5/continue_text.R')
readr::read_lines(path)
paste0(readr::read_lines(path),'\n')
tolower(paste0(readr::read_lines(path),'\n', collapse = ''))
source('C:/Users/qtckp/OneDrive/Рабочий стол/RomanDisease/Нейронка из России/Том 5/continue_text.R')
generated_text
source('C:/Users/qtckp/OneDrive/Рабочий стол/RomanDisease/Нейронка из России/Том 5/continue_text.R')
source('C:/Users/qtckp/OneDrive/Рабочий стол/RomanDisease/Нейронка из России/Том 5/continue_text.R')
source('C:/Users/qtckp/OneDrive/Рабочий стол/RomanDisease/Нейронка из России/Том 5/continue_text.R')
